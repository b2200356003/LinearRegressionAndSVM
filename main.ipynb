{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEPifRWi8RB0"
   },
   "source": [
    "## BBM 409 - Programming Assignment 1\n",
    "\n",
    "* You can add as many cells as you want in-between each question.\n",
    "* Please add comments to your code to explain your work.  \n",
    "* Please add Markdown cells to answer the (non-coding) questions in the homework text. You can, however, refer to the outputs of code cells without adding them as images to the Markdown cell unless you are requested to do otherwise.\n",
    "* Please be careful about the order of runs of cells. Doing the homework, it is likely that you will be running the cells in different orders, however, they will be evaluated in the order they appear. Hence, please try running the cells in this order before submission to make sure they work.    \n",
    "* Please refer to the homework text for any implementation detail. Though you are somewhat expected to abide by the comments in the below cells, they are mainly just provided for guidance. That is, as long as you are not completely off this structure and your work pattern is understandable and traceable, it is fine. For instance, you do not have to implement a particular function within a cell just because the comment directs you to do so.\n",
    "* This document is also your report. Show your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRABL12L8RB4"
   },
   "source": [
    "###  Insert personal information (name, surname, student id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOĞUKAN AYTEKİN 2200356003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qb4QIkS68RB4"
   },
   "source": [
    "## 1. Energy Efficiency Estimation (60 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq8IboH58RB4"
   },
   "source": [
    "### 1.1. Introduction\n",
    "* Brief overview of the regression task.\n",
    "* Description of the dataset used for regression analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction answers \n",
    "* In regression task trying to predicting a continuous output variable based on features. The objective is to understand the relationship between the input variables and the output variables.\n",
    "* This dataset is used to predict building heating and cooling loads based on these features when designing a building or evaluating its energy efficiency. \n",
    "\n",
    "* Features : Relative Compactness,Surface Area,Wall Area,Roof Area,Overall Height,Orientation,Glazing Area,Glazing Area Distribution\n",
    "* Dependent variables : Heating Load , Cooling Load "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-LA2-Ww8RB6"
   },
   "source": [
    "### 1.2. Data Loading and Exploration (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CzXEXAD68RB6"
   },
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable \n",
    "\n",
    "\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mDRlIFmu8RB7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relative_Compactness</th>\n",
       "      <th>Surface_Area</th>\n",
       "      <th>Wall_Area</th>\n",
       "      <th>Roof_Area</th>\n",
       "      <th>Overall_Height</th>\n",
       "      <th>Orientation</th>\n",
       "      <th>Glazing_Area</th>\n",
       "      <th>Glazing_Area_Distribution</th>\n",
       "      <th>Heating_Load</th>\n",
       "      <th>Cooling_Load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.55</td>\n",
       "      <td>21.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>20.84</td>\n",
       "      <td>28.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>17.88</td>\n",
       "      <td>21.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.54</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.44</td>\n",
       "      <td>17.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.48</td>\n",
       "      <td>16.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5</td>\n",
       "      <td>16.64</td>\n",
       "      <td>16.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>768 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Relative_Compactness  Surface_Area  Wall_Area  Roof_Area  Overall_Height  \\\n",
       "0                    0.98         514.5      294.0     110.25             7.0   \n",
       "1                    0.98         514.5      294.0     110.25             7.0   \n",
       "2                    0.98         514.5      294.0     110.25             7.0   \n",
       "3                    0.98         514.5      294.0     110.25             7.0   \n",
       "4                    0.90         563.5      318.5     122.50             7.0   \n",
       "..                    ...           ...        ...        ...             ...   \n",
       "763                  0.64         784.0      343.0     220.50             3.5   \n",
       "764                  0.62         808.5      367.5     220.50             3.5   \n",
       "765                  0.62         808.5      367.5     220.50             3.5   \n",
       "766                  0.62         808.5      367.5     220.50             3.5   \n",
       "767                  0.62         808.5      367.5     220.50             3.5   \n",
       "\n",
       "     Orientation  Glazing_Area  Glazing_Area_Distribution  Heating_Load  \\\n",
       "0              2           0.0                          0         15.55   \n",
       "1              3           0.0                          0         15.55   \n",
       "2              4           0.0                          0         15.55   \n",
       "3              5           0.0                          0         15.55   \n",
       "4              2           0.0                          0         20.84   \n",
       "..           ...           ...                        ...           ...   \n",
       "763            5           0.4                          5         17.88   \n",
       "764            2           0.4                          5         16.54   \n",
       "765            3           0.4                          5         16.44   \n",
       "766            4           0.4                          5         16.48   \n",
       "767            5           0.4                          5         16.64   \n",
       "\n",
       "     Cooling_Load  \n",
       "0           21.33  \n",
       "1           21.33  \n",
       "2           21.33  \n",
       "3           21.33  \n",
       "4           28.28  \n",
       "..            ...  \n",
       "763         21.40  \n",
       "764         16.88  \n",
       "765         17.11  \n",
       "766         16.61  \n",
       "767         16.03  \n",
       "\n",
       "[768 rows x 10 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read the regression data and transform it into a Numpy array collection.\n",
    "## (See pandas and numpy functions)\n",
    "df = pd.read_csv(\"energy_efficiency_data.csv\")\n",
    "data_matrix = df.values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5sqsnzLB8RB7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Relative_Compactness', 'Surface_Area', 'Wall_Area', 'Roof_Area',\n",
      "       'Overall_Height', 'Orientation', 'Glazing_Area',\n",
      "       'Glazing_Area_Distribution', 'Heating_Load', 'Cooling_Load'],\n",
      "      dtype='object')\n",
      "       Relative_Compactness  Surface_Area   Wall_Area   Roof_Area  \\\n",
      "count            768.000000    768.000000  768.000000  768.000000   \n",
      "mean               0.764167    671.708333  318.500000  176.604167   \n",
      "std                0.105777     88.086116   43.626481   45.165950   \n",
      "min                0.620000    514.500000  245.000000  110.250000   \n",
      "25%                0.682500    606.375000  294.000000  140.875000   \n",
      "50%                0.750000    673.750000  318.500000  183.750000   \n",
      "75%                0.830000    741.125000  343.000000  220.500000   \n",
      "max                0.980000    808.500000  416.500000  220.500000   \n",
      "\n",
      "       Overall_Height  Orientation  Glazing_Area  Glazing_Area_Distribution  \\\n",
      "count       768.00000   768.000000    768.000000                  768.00000   \n",
      "mean          5.25000     3.500000      0.234375                    2.81250   \n",
      "std           1.75114     1.118763      0.133221                    1.55096   \n",
      "min           3.50000     2.000000      0.000000                    0.00000   \n",
      "25%           3.50000     2.750000      0.100000                    1.75000   \n",
      "50%           5.25000     3.500000      0.250000                    3.00000   \n",
      "75%           7.00000     4.250000      0.400000                    4.00000   \n",
      "max           7.00000     5.000000      0.400000                    5.00000   \n",
      "\n",
      "       Heating_Load  Cooling_Load  \n",
      "count    768.000000    768.000000  \n",
      "mean      22.307201     24.587760  \n",
      "std       10.090196      9.513306  \n",
      "min        6.010000     10.900000  \n",
      "25%       12.992500     15.620000  \n",
      "50%       18.950000     22.080000  \n",
      "75%       31.667500     33.132500  \n",
      "max       43.100000     48.030000  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 10 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Relative_Compactness       768 non-null    float64\n",
      " 1   Surface_Area               768 non-null    float64\n",
      " 2   Wall_Area                  768 non-null    float64\n",
      " 3   Roof_Area                  768 non-null    float64\n",
      " 4   Overall_Height             768 non-null    float64\n",
      " 5   Orientation                768 non-null    int64  \n",
      " 6   Glazing_Area               768 non-null    float64\n",
      " 7   Glazing_Area_Distribution  768 non-null    int64  \n",
      " 8   Heating_Load               768 non-null    float64\n",
      " 9   Cooling_Load               768 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 60.1 KB\n"
     ]
    }
   ],
   "source": [
    "## Explore the dataset (e.g., size, features, target variables, summary statistics).\n",
    "## Check for any missing values and handle them if necessary.\n",
    "shape = df.shape\n",
    "print(df.columns)\n",
    "print(df.describe())\n",
    "# not any NaN values\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ge1YGgKT8RB7"
   },
   "source": [
    "### 1.3. Data Preprocessing (10 points)\n",
    "* Explain the preprocessing steps taken and their rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apply chi square and correlation feature selection methods and for feature scaling standardization and normalization techniques are used.\n",
    "\n",
    "I will explain the rationale between my choice of methods in comparing table of feature scaling and feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZJ745n3N8RB7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heating Load chi2 scores\n",
      "Relative_Compactness   7439.199999999999\n",
      "Surface_Area   7439.199999999999\n",
      "Wall_Area   3998.133333333333\n",
      "Roof_Area   2168.833333333333\n",
      "Overall_Height   768.0\n",
      "Orientation   1765.8666666666668\n",
      "Glazing_Area   2098.453333333333\n",
      "Glazing_Area_Distribution   3108.977777777778\n",
      "Heating_Load   449280.00000000006\n",
      "Cooling_Load   381368.53333333344\n",
      "--------------------\n",
      "Cooling Load chi2 scores\n",
      "Relative_Compactness   7462.0\n",
      "Surface_Area   7462.0\n",
      "Wall_Area   4002.0\n",
      "Roof_Area   2136.3333333333335\n",
      "Overall_Height   768.0\n",
      "Orientation   1928.0\n",
      "Glazing_Area   2176.0\n",
      "Glazing_Area_Distribution   3277.333333333334\n",
      "Heating_Load   381368.53333333344\n",
      "Cooling_Load   487679.99999999994\n",
      "--------------------\n",
      "Relative_Compactness         0.622272\n",
      "Surface_Area                -0.658120\n",
      "Wall_Area                    0.455671\n",
      "Roof_Area                   -0.861828\n",
      "Overall_Height               0.889431\n",
      "Orientation                 -0.002587\n",
      "Glazing_Area                 0.269841\n",
      "Glazing_Area_Distribution    0.087368\n",
      "Heating_Load                 1.000000\n",
      "Cooling_Load                 0.975862\n",
      "Name: Heating_Load, dtype: float64\n",
      "Relative_Compactness         0.634339\n",
      "Surface_Area                -0.672999\n",
      "Wall_Area                    0.427117\n",
      "Roof_Area                   -0.862547\n",
      "Overall_Height               0.895785\n",
      "Orientation                  0.014290\n",
      "Glazing_Area                 0.207505\n",
      "Glazing_Area_Distribution    0.050525\n",
      "Heating_Load                 0.975862\n",
      "Cooling_Load                 1.000000\n",
      "Name: Cooling_Load, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Apply feature selection methods to see if it has a contribution on performance. (ablation study)\n",
    "## Implement functions for different feature scaling techniques to see their effects on the performance. (ablation study)\n",
    "## Explain the rationale your choice of methods.\n",
    "\n",
    "# using chi square method as feature selection method. Lower than 3000 not selected \n",
    "\n",
    "print('Heating Load chi2 scores')\n",
    "for column in df.columns.tolist():\n",
    "    cross_tab = pd.crosstab(df['Heating_Load'], df[column])\n",
    "    chi2, p_value, _, _ = chi2_contingency(cross_tab)\n",
    "    print(column , \" \" , chi2)\n",
    "\n",
    "df_heat_chi = df.drop(['Roof_Area' , 'Overall_Height' , 'Glazing_Area' ,'Orientation' , 'Cooling_Load'] , axis = 1)\n",
    "\n",
    "df_heat_chi = df_heat_chi.drop_duplicates()\n",
    "df_heat_chi_matrix = df_heat_chi.values\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "print('Cooling Load chi2 scores')\n",
    "for column in df.columns.tolist():\n",
    "    cross_tab = pd.crosstab(df['Cooling_Load'], df[column])\n",
    "    chi2, p_value, _, _ = chi2_contingency(cross_tab)\n",
    "    print(column , \" \" , chi2)\n",
    "\n",
    "df_cool_chi = df.drop(['Roof_Area' , 'Overall_Height' , 'Glazing_Area' ,'Orientation' , 'Heating_Load'] , axis = 1)\n",
    "\n",
    "df_cool_chi = df_cool_chi.drop_duplicates()\n",
    "df_cool_chi_matrix = df_cool_chi.values\n",
    "\n",
    "\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# Using corelation as feature selection methods lower than 0.3 not selected\n",
    "\n",
    "print(df.corr()['Heating_Load'])\n",
    "\n",
    "# preparing dataFrame for heating_Load prediction\n",
    "Heating = df['Heating_Load']\n",
    "df_heat_correlation = df.drop(['Orientation' , 'Glazing_Area' , 'Glazing_Area_Distribution' , 'Heating_Load' , 'Cooling_Load'] , axis = 1)\n",
    "df_heat_correlation['Heating'] = Heating\n",
    "\n",
    "\n",
    "# remove duplicates\n",
    "df_heat_correlation = df_heat_correlation.drop_duplicates()\n",
    "\n",
    "#create np matrix\n",
    "df_heat_correlation['Overall_Height']\n",
    "df_heat_correlation_matrix = df_heat_correlation.values\n",
    "\n",
    "# same steps applied for cooling load predict\n",
    "print(df.corr()['Cooling_Load'])\n",
    "\n",
    "df_cool_correlation = df.drop(['Orientation' , 'Glazing_Area' , 'Glazing_Area_Distribution' , 'Heating_Load'] , axis = 1)\n",
    "\n",
    "df_cool_correlation = df_cool_correlation.drop_duplicates()\n",
    "df_cool_correlation_matrix = df_cool_correlation.values\n",
    "\n",
    "# Feature scaling methods pass column by column\n",
    "def normalize(matrix):\n",
    "    min_vals = np.min(matrix, axis=0)\n",
    "    max_vals = np.max(matrix, axis=0)\n",
    "    normalized_matrix = (matrix - min_vals) / (max_vals - min_vals)\n",
    "    return normalized_matrix\n",
    "\n",
    "def standardize(matrix):\n",
    "    mean_vals = np.mean(matrix, axis=0)\n",
    "    std_vals = np.std(matrix, axis=0)\n",
    "    standardized_matrix = (matrix - mean_vals) / std_vals\n",
    "    return standardized_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Implementation Detail \n",
    "* Teacher you can change the working_matrix variable with one of this matrices (df_heat_correlation_matrix , df_heat_chi_matrix , df_cool_correlation_matrix , df_cool_chi_matrix ) to see the performance of that type of prediction. \n",
    "\n",
    "* Before the result and analysis part I create a table to compare all of the predictions performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_matrix = df_heat_correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can see the results of chi square and correlation tests. We analized and select the features with the spesific thresholds about that values at the previouse code section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mlTxWFCBm9Tg"
   },
   "outputs": [],
   "source": [
    "## Handle missing values (if any).\n",
    "# no missing values\n",
    "\n",
    "## Split the dataset into training and testing sets.\n",
    "## Set the training set to be 80% and the test set to be 20% of the dataset.\n",
    "\n",
    "\n",
    "def SplitAndReshape(matrix,scaleNumber):\n",
    "\n",
    "    # we are giving scaleNumber to choose the feature scaling method (-1 -> no scaling) , (0 -> normalizing) , (1 -> standardizing)\n",
    "\n",
    "    colMinusOne = matrix.shape[1]-1\n",
    "    if(scaleNumber!=-1):\n",
    "        for i in range(colMinusOne):\n",
    "            if (scaleNumber==0):\n",
    "                matrix[:,i]= normalize(matrix[:,i])\n",
    "            else:\n",
    "                matrix[:,i]= standardize(matrix[:,i])\n",
    "\n",
    "    number_of_rows = matrix.shape[0]\n",
    "    number_of_columns = matrix.shape[1]\n",
    "\n",
    "    # spliting the test and train data\n",
    "\n",
    "    train_size = int(4*(number_of_rows/5))\n",
    "    test_size = number_of_rows-train_size\n",
    "\n",
    "    train_data = matrix[:train_size]\n",
    "    x_train , y_train = train_data[:,:colMinusOne] , train_data[:,colMinusOne]\n",
    "\n",
    "    test_data = matrix[train_size:]\n",
    "    x_test , y_test = test_data[:,:colMinusOne] , test_data[:,colMinusOne]\n",
    "\n",
    "    # stack feature matrixes first column with all ones to multiply with bias\n",
    "    x_train = np.vstack((np.ones((x_train.shape[0],)), x_train.T)).T\n",
    "    x_test = np.vstack((np.ones((x_test.shape[0],)), x_test.T)).T\n",
    "\n",
    "    y_train = np.reshape(y_train,(y_train.size,1))\n",
    "    y_test = np.reshape(y_test,(y_test.size,1))\n",
    "\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "x_train,y_train,x_test,y_test = SplitAndReshape(working_matrix,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In this previous section we first scale our feature values and reshape the y values to apply our model properly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the rationale behind the chosen split ratio.\n",
    "Splitting the training and test datasets into 80-20 ratio provides model to learns effiecently from training data and having enough data to objectively evaluate its generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6yQkfFD8RB8"
   },
   "source": [
    "### 1.4. Linear Regression Model (15 points)\n",
    "* Explain the reason behind the application of linear regression on this type of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jYEoFWPJoZ-Y"
   },
   "outputs": [],
   "source": [
    "## Implement linear regression model from scratch, using libraries like NumPy.\n",
    "def model (X,Y, learning_rate , iteration , x_mse, y_mse):\n",
    "    mse_list = []\n",
    "    weights = np.zeros((X.shape[1],1))\n",
    "\n",
    "    for i in range(iteration):\n",
    "        x_train_transpose = X.T\n",
    "        y_pred = np.dot(X,weights)\n",
    "\n",
    "        # formula of differential of the error function\n",
    "        diff_of_error = (1/Y.size) * np.dot(x_train_transpose, y_pred - Y)\n",
    "\n",
    "        # update weights\n",
    "        weights = weights - (learning_rate * diff_of_error)\n",
    "\n",
    "\n",
    "        # creating list of MSE's to plot the curve\n",
    "        if(i%10 == 0):\n",
    "            y_pred = np.dot(x_mse, weights)\n",
    "            mse_test = np.mean(np.square(y_mse - y_pred))\n",
    "            mse_list.append(mse_test)\n",
    "\n",
    "    return mse_list , weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_G5n-KKRrg0b"
   },
   "outputs": [],
   "source": [
    "def TrainTestAndCompare(iteration, learning_rate):\n",
    "    ## Train the model using the training dataset.\n",
    "\n",
    "    mse_list , weights = model(x_train,y_train,learning_rate,iteration, x_train, y_train)\n",
    "\n",
    "    ## Evaluate the model's performance on the training set by computing MSE.\n",
    "    y_pred = np.dot(x_train, weights)\n",
    "    average_fault_value = np.mean(np.abs(y_train-y_pred))\n",
    "    mse_train = np.mean(np.square(y_train - y_pred))\n",
    "    average_truth_value = np.mean(y_train)\n",
    "    \n",
    "    return mse_list , weights , average_fault_value , average_truth_value , mse_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's try different iterations and learning rate values to try to find the minimum MSE and for compare I will create a Table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "le1Pd9Qg8RB8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+---------------------+---------------------+--------------------+\n",
      "| Number of Iterations | Learning Rate | Average Fault Value | Average Truth Value |        MSE         |\n",
      "+----------------------+---------------+---------------------+---------------------+--------------------+\n",
      "|         500          |     0.0001    |  21.269890047445234 |  22.382438095238097 | 529.6290364902566  |\n",
      "|         500          |     0.001     |  13.515067088409523 |  22.382438095238097 | 208.0976125303798  |\n",
      "|         500          |      0.01     |  3.1333180421051847 |  22.382438095238097 | 17.850332664085528 |\n",
      "|         1000         |     0.0001    |  20.21564534386332  |  22.382438095238097 | 470.43548462664137 |\n",
      "|         1000         |     0.001     |  8.308296403751546  |  22.382438095238097 | 87.82980839664773  |\n",
      "|         1000         |      0.01     |  3.0821400507850405 |  22.382438095238097 | 17.076002938400563 |\n",
      "|         2000         |     0.0001    |  18.26815663673505  |  22.382438095238097 | 376.8974892968902  |\n",
      "|         2000         |     0.001     |  4.037944595100994  |  22.382438095238097 | 27.88149087201596  |\n",
      "|         2000         |      0.01     |  3.017623272175884  |  22.382438095238097 | 16.388938244660398 |\n",
      "|         5000         |     0.0001    |  13.518143947490337 |  22.382438095238097 | 208.19291488043115 |\n",
      "|         5000         |     0.001     |  3.133338753744693  |  22.382438095238097 | 17.851609240892902 |\n",
      "|         5000         |      0.01     |  3.0006756121018876 |  22.382438095238097 | 16.026794267057358 |\n",
      "+----------------------+---------------+---------------------+---------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "iteration_list = [500,1000,2000,5000]\n",
    "learning_rate_list = [0.0001,0.001,0.01]\n",
    "myTable_1 = PrettyTable([\"Number of Iterations\", \"Learning Rate\",\"Average Fault Value\", \"Average Truth Value\" ,\"MSE\"])\n",
    "\n",
    "for iteration in iteration_list:\n",
    "    for learning_rate in learning_rate_list:\n",
    "        mse_list , weights , fault , truth , MSE_ = TrainTestAndCompare(iteration,learning_rate)\n",
    "        myTable_1.add_row([iteration, learning_rate, fault , truth , MSE_])\n",
    "\n",
    "print(myTable_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see from table the minimum MSE comes when we pick number of iterations 5000 and learning rate 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a table to compare the model performance between different iteration and learning rate values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23njcboArttc"
   },
   "source": [
    "### 1.5 Model Evaluation on Test Set (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Z7dmNGprxDo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fault value: 3.8517553416590578\n",
      "Average truth value 25.707348484848485\n",
      "MSE value in test data :  21.401039242804003\n"
     ]
    }
   ],
   "source": [
    "## Make predictions on the test set using the trained model.\n",
    "## Calculate Mean Squared Error (MSE) for the test set.\n",
    "## Comment on the scores.\n",
    "\n",
    "# calculating MSE for test set\n",
    "y_pred = np.dot(x_test, weights)\n",
    "print(\"Average fault value:\" , np.mean(np.abs(y_test-y_pred)))\n",
    "print(\"Average truth value\" ,np.mean(y_test))\n",
    "mse_test = np.mean(np.square(y_test - y_pred))\n",
    "print(\"MSE value in test data : \" ,mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the scores of both test and train data test MSE is slight bigger than train data as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dDG_al2zr76F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------+--------------------------+--------------------+\n",
      "| Predicted Feature | Feature Scaling Method | Feature Selection Method |        MSE         |\n",
      "+-------------------+------------------------+--------------------------+--------------------+\n",
      "|    Heating_Load   |     Normalization      |       Correlation        | 21.741841930780325 |\n",
      "|    Heating_Load   |    Standardization     |       Correlation        | 21.401039242804014 |\n",
      "|    Heating_Load   |     Normalization      |        Chi Square        | 25.42130687038043  |\n",
      "|    Heating_Load   |    Standardization     |        Chi Square        | 21.696548112142754 |\n",
      "|    Cooling_Load   |     Normalization      |       Correlation        | 16.144294311766373 |\n",
      "|    Cooling_Load   |    Standardization     |       Correlation        | 15.717026981317701 |\n",
      "|    Cooling_Load   |     Normalization      |        Chi Square        | 22.930897740032147 |\n",
      "|    Cooling_Load   |    Standardization     |        Chi Square        | 19.180025940773625 |\n",
      "+-------------------+------------------------+--------------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "## Create a table to present the test MSEs for different feature scaling and selection methods.\n",
    "\n",
    "# calculating MSE 's for different feature scaling and selection methods in same iteration and learning rate values. \n",
    "\n",
    "iteration = 5000\n",
    "learning_rate = 0.01\n",
    "mse_scores_list = []\n",
    "\n",
    "def calculateMse(df_matrix,scaling_score,weights):\n",
    "    x_train,y_train,x_test,y_test = SplitAndReshape(df_matrix,scaling_score)\n",
    "    mse_list , weights = model(x_train,y_train,learning_rate,iteration, x_train, y_train)\n",
    "    y_pred = np.dot(x_test, weights)\n",
    "    mse = np.mean(np.square(y_test - y_pred))\n",
    "    return mse\n",
    "\n",
    "mse_scores_list.append(calculateMse(df_heat_correlation_matrix,0,weights)) # normalized\n",
    "mse_scores_list.append(calculateMse(df_heat_correlation_matrix,1,weights)) # standardized\n",
    "mse_scores_list.append(calculateMse(df_heat_chi_matrix,0,weights)) # normalized\n",
    "mse_scores_list.append(calculateMse(df_heat_chi_matrix,1,weights)) # standardized\n",
    "\n",
    "mse_scores_list.append(calculateMse(df_cool_correlation_matrix,0,weights)) # normalized\n",
    "mse_scores_list.append(calculateMse(df_cool_correlation_matrix,1,weights)) # standardized\n",
    "mse_scores_list.append(calculateMse(df_cool_chi_matrix,0,weights)) # normalized\n",
    "mse_scores_list.append(calculateMse(df_cool_chi_matrix,1,weights)) # standardized\n",
    "\n",
    "myTable = PrettyTable([\"Predicted Feature\", \"Feature Scaling Method\",\"Feature Selection Method\" ,\"MSE\"])\n",
    "myTable.add_row([\"Heating_Load\", \"Normalization\",\"Correlation\" ,mse_scores_list[0]]) \n",
    "myTable.add_row([\"Heating_Load\", \"Standardization\", \"Correlation\",mse_scores_list[1]]) \n",
    "\n",
    "myTable.add_row([\"Heating_Load\", \"Normalization\",\"Chi Square\" ,mse_scores_list[2]]) \n",
    "myTable.add_row([\"Heating_Load\", \"Standardization\", \"Chi Square\",mse_scores_list[3]]) \n",
    "\n",
    "myTable.add_row([\"Cooling_Load\", \"Normalization\",\"Correlation\"  ,mse_scores_list[4]]) \n",
    "myTable.add_row([\"Cooling_Load\", \"Standardization\",\"Correlation\"  ,mse_scores_list[5]]) \n",
    "\n",
    "myTable.add_row([\"Cooling_Load\", \"Normalization\",\"Chi Square\" ,mse_scores_list[6]]) \n",
    "myTable.add_row([\"Cooling_Load\", \"Standardization\",\"Chi Square\" ,mse_scores_list[7]]) \n",
    "\n",
    "print(myTable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As we can see on table in both predicted features , best methods are Standardization and Correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3aW87Td98RB9"
   },
   "source": [
    "### 1.6 Results Analysis and Conclusion (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q3DK67Y8oyKr"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot training MSEs to visualize the learning curve.\n",
    "\n",
    "index_list = []\n",
    "for i in range(len(mse_list)):\n",
    "    index_list.append(i)\n",
    "\n",
    "plt.title(\"Training MSE Curve\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Value')\n",
    "plt.plot(index_list,mse_list, color=\"black\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we look at the curve our MSE value is decreasing continously so we can say that our model is working appropiately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the results of the ablation study on feature scaling and selection.\n",
    "\n",
    "Applying feature scaling technique reduces the numerical differences between the values of our features, leading to more accurate results from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize key findings from the regression analysis.\n",
    "\n",
    "* In this regression analysis task first we analyze our data then we calculate the chi square and correlation values to select the features and scale them with standardization and normalization techniques . Then we coded our model from scratch. Lastly we train our model with data matrixes which created with different feature scaling and feature selection methods and test them.\n",
    "\n",
    "* If we look at the scores in test set averagely our predicted value is 22.3 and the fault is 3.0 I think this is an acceptable result for this model. \n",
    "\n",
    "* Also if we talk about the feature selection and scaling methods . We can easily see that correlation for feature selection and standardization for feature scaling works better for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jfa8FPbS8RCC"
   },
   "source": [
    "## 2. Personality Type Classification (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70saVqfas_xw"
   },
   "source": [
    "### 2.1. Introduction\n",
    "* Brief overview of the classification task.\n",
    "* Description of the dataset used for classification analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In classifiaction task our aim is to label the data points correctly\n",
    "\n",
    "* In our classification dataset we have some question answers and with that answers we are trying to guess the personality type of the person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Y4hOS8Js_xw"
   },
   "source": [
    "### 2.2. Data Loading and Exploration (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Y92cEVTIs_xw"
   },
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "grKg2iiAs_xw"
   },
   "outputs": [],
   "source": [
    "## Read the classification data and transform it into a Numpy array collection.\n",
    "## (See pandas and numpy functions)\n",
    "df_personality = pd.read_csv(\"subset_16P.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p7IeqyCNs_xw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Response Id', 'You regularly make new friends.',\n",
      "       'You spend a lot of your free time exploring various random topics that pique your interest',\n",
      "       'Seeing other people cry can easily make you feel like you want to cry too',\n",
      "       'You often make a backup plan for a backup plan.',\n",
      "       'You usually stay calm, even under a lot of pressure',\n",
      "       'At social events, you rarely try to introduce yourself to new people and mostly talk to the ones you already know',\n",
      "       'You prefer to completely finish one project before starting another.',\n",
      "       'You are very sentimental.',\n",
      "       'You like to use organizing tools like schedules and lists.',\n",
      "       'Even a small mistake can cause you to doubt your overall abilities and knowledge.',\n",
      "       'You feel comfortable just walking up to someone you find interesting and striking up a conversation.',\n",
      "       'You are not too interested in discussing various interpretations and analyses of creative works.',\n",
      "       'You are more inclined to follow your head than your heart.',\n",
      "       'You usually prefer just doing what you feel like at any given moment instead of planning a particular daily routine.',\n",
      "       'You rarely worry about whether you make a good impression on people you meet.',\n",
      "       'You enjoy participating in group activities.',\n",
      "       'You like books and movies that make you come up with your own interpretation of the ending.',\n",
      "       'Your happiness comes more from helping others accomplish things than your own accomplishments.',\n",
      "       'You are interested in so many things that you find it difficult to choose what to try next.',\n",
      "       'You are prone to worrying that things will take a turn for the worse.',\n",
      "       'You avoid leadership roles in group settings.',\n",
      "       'You are definitely not an artistic type of person.',\n",
      "       'You think the world would be a better place if people relied more on rationality and less on their feelings.',\n",
      "       'You prefer to do your chores before allowing yourself to relax.',\n",
      "       'You enjoy watching people argue.',\n",
      "       'You tend to avoid drawing attention to yourself.',\n",
      "       'Your mood can change very quickly.',\n",
      "       'You lose patience with people who are not as efficient as you.',\n",
      "       'You often end up doing things at the last possible moment.',\n",
      "       'You have always been fascinated by the question of what, if anything, happens after death.',\n",
      "       'You usually prefer to be around others rather than on your own.',\n",
      "       'You become bored or lose interest when the discussion gets highly theoretical.',\n",
      "       'You find it easy to empathize with a person whose experiences are very different from yours.',\n",
      "       'You usually postpone finalizing decisions for as long as possible.',\n",
      "       'You rarely second-guess the choices that you have made.',\n",
      "       'After a long and exhausting week, a lively social event is just what you need.',\n",
      "       'You enjoy going to art museums.',\n",
      "       'You often have a hard time understanding other people’s feelings.',\n",
      "       'You like to have a to-do list for each day.',\n",
      "       'You rarely feel insecure.', 'You avoid making phone calls.',\n",
      "       'You often spend a lot of time trying to understand views that are very different from your own.',\n",
      "       'In your social circle, you are often the one who contacts your friends and initiates activities.',\n",
      "       'If your plans are interrupted, your top priority is to get back on track as soon as possible.',\n",
      "       'You are still bothered by mistakes that you made a long time ago.',\n",
      "       'You rarely contemplate the reasons for human existence or the meaning of life.',\n",
      "       'Your emotions control you more than you control them.',\n",
      "       'You take great care not to make people look bad, even when it is completely their fault.',\n",
      "       'Your personal work style is closer to spontaneous bursts of energy than organized and consistent efforts.',\n",
      "       'When someone thinks highly of you, you wonder how long it will take them to feel disappointed in you.',\n",
      "       'You would love a job that requires you to work alone most of the time.',\n",
      "       'You believe that pondering abstract philosophical questions is a waste of time.',\n",
      "       'You feel more drawn to places with busy, bustling atmospheres than quiet, intimate places.',\n",
      "       'You know at first glance how someone is feeling.',\n",
      "       'You often feel overwhelmed.',\n",
      "       'You complete things methodically without skipping over any steps.',\n",
      "       'You are very intrigued by things labeled as controversial.',\n",
      "       'You would pass along a good opportunity if you thought someone else needed it more.',\n",
      "       'You struggle with deadlines.',\n",
      "       'You feel confident that things will work out for you.', 'Personality'],\n",
      "      dtype='object')\n",
      "        Response Id  You regularly make new friends.  \\\n",
      "count  10000.000000                      10000.00000   \n",
      "mean   30033.526600                         -0.00420   \n",
      "std    17310.103985                          0.37013   \n",
      "min        0.000000                         -1.00000   \n",
      "25%    15058.750000                          0.00000   \n",
      "50%    29961.500000                          0.00000   \n",
      "75%    45206.750000                          0.00000   \n",
      "max    59997.000000                          1.00000   \n",
      "\n",
      "       You spend a lot of your free time exploring various random topics that pique your interest  \\\n",
      "count                                       10000.000000                                            \n",
      "mean                                            0.002100                                            \n",
      "std                                             0.370013                                            \n",
      "min                                            -1.000000                                            \n",
      "25%                                             0.000000                                            \n",
      "50%                                             0.000000                                            \n",
      "75%                                             0.000000                                            \n",
      "max                                             1.000000                                            \n",
      "\n",
      "       Seeing other people cry can easily make you feel like you want to cry too  \\\n",
      "count                                        10000.00000                           \n",
      "mean                                             0.01470                           \n",
      "std                                              1.53796                           \n",
      "min                                             -3.00000                           \n",
      "25%                                             -1.00000                           \n",
      "50%                                              0.00000                           \n",
      "75%                                              1.00000                           \n",
      "max                                              3.00000                           \n",
      "\n",
      "       You often make a backup plan for a backup plan.  \\\n",
      "count                                     10000.000000   \n",
      "mean                                         -0.211000   \n",
      "std                                           1.523388   \n",
      "min                                          -3.000000   \n",
      "25%                                          -1.000000   \n",
      "50%                                           0.000000   \n",
      "75%                                           1.000000   \n",
      "max                                           3.000000   \n",
      "\n",
      "       You usually stay calm, even under a lot of pressure  \\\n",
      "count                                        10000.00000     \n",
      "mean                                            -0.14970     \n",
      "std                                              1.49416     \n",
      "min                                             -3.00000     \n",
      "25%                                             -1.00000     \n",
      "50%                                              0.00000     \n",
      "75%                                              1.00000     \n",
      "max                                              3.00000     \n",
      "\n",
      "       At social events, you rarely try to introduce yourself to new people and mostly talk to the ones you already know  \\\n",
      "count                                       10000.000000                                                                   \n",
      "mean                                            0.012500                                                                   \n",
      "std                                             1.514983                                                                   \n",
      "min                                            -3.000000                                                                   \n",
      "25%                                            -1.000000                                                                   \n",
      "50%                                             0.000000                                                                   \n",
      "75%                                             1.000000                                                                   \n",
      "max                                             3.000000                                                                   \n",
      "\n",
      "       You prefer to completely finish one project before starting another.  \\\n",
      "count                                        10000.00000                      \n",
      "mean                                            -0.45950                      \n",
      "std                                              1.45278                      \n",
      "min                                             -3.00000                      \n",
      "25%                                             -2.00000                      \n",
      "50%                                             -1.00000                      \n",
      "75%                                              0.00000                      \n",
      "max                                              3.00000                      \n",
      "\n",
      "       You are very sentimental.  \\\n",
      "count               10000.000000   \n",
      "mean                    0.002400   \n",
      "std                     0.362777   \n",
      "min                    -1.000000   \n",
      "25%                     0.000000   \n",
      "50%                     0.000000   \n",
      "75%                     0.000000   \n",
      "max                     1.000000   \n",
      "\n",
      "       You like to use organizing tools like schedules and lists.  ...  \\\n",
      "count                                       10000.000000           ...   \n",
      "mean                                            0.130300           ...   \n",
      "std                                             1.535629           ...   \n",
      "min                                            -3.000000           ...   \n",
      "25%                                            -1.000000           ...   \n",
      "50%                                             0.000000           ...   \n",
      "75%                                             1.000000           ...   \n",
      "max                                             3.000000           ...   \n",
      "\n",
      "       You would love a job that requires you to work alone most of the time.  \\\n",
      "count                                       10000.000000                        \n",
      "mean                                            0.007700                        \n",
      "std                                             0.358962                        \n",
      "min                                            -1.000000                        \n",
      "25%                                             0.000000                        \n",
      "50%                                             0.000000                        \n",
      "75%                                             0.000000                        \n",
      "max                                             1.000000                        \n",
      "\n",
      "       You believe that pondering abstract philosophical questions is a waste of time.  \\\n",
      "count                                       10000.000000                                 \n",
      "mean                                            0.000700                                 \n",
      "std                                             0.364572                                 \n",
      "min                                            -1.000000                                 \n",
      "25%                                             0.000000                                 \n",
      "50%                                             0.000000                                 \n",
      "75%                                             0.000000                                 \n",
      "max                                             1.000000                                 \n",
      "\n",
      "       You feel more drawn to places with busy, bustling atmospheres than quiet, intimate places.  \\\n",
      "count                                       10000.000000                                            \n",
      "mean                                            0.123400                                            \n",
      "std                                             1.528073                                            \n",
      "min                                            -3.000000                                            \n",
      "25%                                            -1.000000                                            \n",
      "50%                                             0.000000                                            \n",
      "75%                                             1.000000                                            \n",
      "max                                             3.000000                                            \n",
      "\n",
      "       You know at first glance how someone is feeling.  \\\n",
      "count                                      10000.000000   \n",
      "mean                                          -0.002900   \n",
      "std                                            0.371087   \n",
      "min                                           -1.000000   \n",
      "25%                                            0.000000   \n",
      "50%                                            0.000000   \n",
      "75%                                            0.000000   \n",
      "max                                            1.000000   \n",
      "\n",
      "       You often feel overwhelmed.  \\\n",
      "count                 10000.000000   \n",
      "mean                      0.258500   \n",
      "std                       1.495494   \n",
      "min                      -3.000000   \n",
      "25%                      -1.000000   \n",
      "50%                       0.000000   \n",
      "75%                       1.000000   \n",
      "max                       3.000000   \n",
      "\n",
      "       You complete things methodically without skipping over any steps.  \\\n",
      "count                                       10000.000000                   \n",
      "mean                                           -0.004600                   \n",
      "std                                             0.363857                   \n",
      "min                                            -1.000000                   \n",
      "25%                                             0.000000                   \n",
      "50%                                             0.000000                   \n",
      "75%                                             0.000000                   \n",
      "max                                             1.000000                   \n",
      "\n",
      "       You are very intrigued by things labeled as controversial.  \\\n",
      "count                                       10000.000000            \n",
      "mean                                           -0.002400            \n",
      "std                                             0.368792            \n",
      "min                                            -1.000000            \n",
      "25%                                             0.000000            \n",
      "50%                                             0.000000            \n",
      "75%                                             0.000000            \n",
      "max                                             1.000000            \n",
      "\n",
      "       You would pass along a good opportunity if you thought someone else needed it more.  \\\n",
      "count                                         10000.0000                                     \n",
      "mean                                              0.1192                                     \n",
      "std                                               1.5250                                     \n",
      "min                                              -3.0000                                     \n",
      "25%                                              -1.0000                                     \n",
      "50%                                               0.0000                                     \n",
      "75%                                               1.0000                                     \n",
      "max                                               3.0000                                     \n",
      "\n",
      "       You struggle with deadlines.  \\\n",
      "count                  10000.000000   \n",
      "mean                      -0.027200   \n",
      "std                        1.531305   \n",
      "min                       -3.000000   \n",
      "25%                       -1.000000   \n",
      "50%                        0.000000   \n",
      "75%                        1.000000   \n",
      "max                        3.000000   \n",
      "\n",
      "       You feel confident that things will work out for you.  \n",
      "count                                       10000.000000      \n",
      "mean                                            0.100300      \n",
      "std                                             1.561885      \n",
      "min                                            -3.000000      \n",
      "25%                                            -1.000000      \n",
      "50%                                             0.000000      \n",
      "75%                                             1.000000      \n",
      "max                                             3.000000      \n",
      "\n",
      "[8 rows x 61 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 62 columns):\n",
      " #   Column                                                                                                                Non-Null Count  Dtype \n",
      "---  ------                                                                                                                --------------  ----- \n",
      " 0   Response Id                                                                                                           10000 non-null  int64 \n",
      " 1   You regularly make new friends.                                                                                       10000 non-null  int64 \n",
      " 2   You spend a lot of your free time exploring various random topics that pique your interest                            10000 non-null  int64 \n",
      " 3   Seeing other people cry can easily make you feel like you want to cry too                                             10000 non-null  int64 \n",
      " 4   You often make a backup plan for a backup plan.                                                                       10000 non-null  int64 \n",
      " 5   You usually stay calm, even under a lot of pressure                                                                   10000 non-null  int64 \n",
      " 6   At social events, you rarely try to introduce yourself to new people and mostly talk to the ones you already know     10000 non-null  int64 \n",
      " 7   You prefer to completely finish one project before starting another.                                                  10000 non-null  int64 \n",
      " 8   You are very sentimental.                                                                                             10000 non-null  int64 \n",
      " 9   You like to use organizing tools like schedules and lists.                                                            10000 non-null  int64 \n",
      " 10  Even a small mistake can cause you to doubt your overall abilities and knowledge.                                     10000 non-null  int64 \n",
      " 11  You feel comfortable just walking up to someone you find interesting and striking up a conversation.                  10000 non-null  int64 \n",
      " 12  You are not too interested in discussing various interpretations and analyses of creative works.                      10000 non-null  int64 \n",
      " 13  You are more inclined to follow your head than your heart.                                                            10000 non-null  int64 \n",
      " 14  You usually prefer just doing what you feel like at any given moment instead of planning a particular daily routine.  10000 non-null  int64 \n",
      " 15  You rarely worry about whether you make a good impression on people you meet.                                         10000 non-null  int64 \n",
      " 16  You enjoy participating in group activities.                                                                          10000 non-null  int64 \n",
      " 17  You like books and movies that make you come up with your own interpretation of the ending.                           10000 non-null  int64 \n",
      " 18  Your happiness comes more from helping others accomplish things than your own accomplishments.                        10000 non-null  int64 \n",
      " 19  You are interested in so many things that you find it difficult to choose what to try next.                           10000 non-null  int64 \n",
      " 20  You are prone to worrying that things will take a turn for the worse.                                                 10000 non-null  int64 \n",
      " 21  You avoid leadership roles in group settings.                                                                         10000 non-null  int64 \n",
      " 22  You are definitely not an artistic type of person.                                                                    10000 non-null  int64 \n",
      " 23  You think the world would be a better place if people relied more on rationality and less on their feelings.          10000 non-null  int64 \n",
      " 24  You prefer to do your chores before allowing yourself to relax.                                                       10000 non-null  int64 \n",
      " 25  You enjoy watching people argue.                                                                                      10000 non-null  int64 \n",
      " 26  You tend to avoid drawing attention to yourself.                                                                      10000 non-null  int64 \n",
      " 27  Your mood can change very quickly.                                                                                    10000 non-null  int64 \n",
      " 28  You lose patience with people who are not as efficient as you.                                                        10000 non-null  int64 \n",
      " 29  You often end up doing things at the last possible moment.                                                            10000 non-null  int64 \n",
      " 30  You have always been fascinated by the question of what, if anything, happens after death.                            10000 non-null  int64 \n",
      " 31  You usually prefer to be around others rather than on your own.                                                       10000 non-null  int64 \n",
      " 32  You become bored or lose interest when the discussion gets highly theoretical.                                        10000 non-null  int64 \n",
      " 33  You find it easy to empathize with a person whose experiences are very different from yours.                          10000 non-null  int64 \n",
      " 34  You usually postpone finalizing decisions for as long as possible.                                                    10000 non-null  int64 \n",
      " 35  You rarely second-guess the choices that you have made.                                                               10000 non-null  int64 \n",
      " 36  After a long and exhausting week, a lively social event is just what you need.                                        10000 non-null  int64 \n",
      " 37  You enjoy going to art museums.                                                                                       10000 non-null  int64 \n",
      " 38  You often have a hard time understanding other people’s feelings.                                                     10000 non-null  int64 \n",
      " 39  You like to have a to-do list for each day.                                                                           10000 non-null  int64 \n",
      " 40  You rarely feel insecure.                                                                                             10000 non-null  int64 \n",
      " 41  You avoid making phone calls.                                                                                         10000 non-null  int64 \n",
      " 42  You often spend a lot of time trying to understand views that are very different from your own.                       10000 non-null  int64 \n",
      " 43  In your social circle, you are often the one who contacts your friends and initiates activities.                      10000 non-null  int64 \n",
      " 44  If your plans are interrupted, your top priority is to get back on track as soon as possible.                         10000 non-null  int64 \n",
      " 45  You are still bothered by mistakes that you made a long time ago.                                                     10000 non-null  int64 \n",
      " 46  You rarely contemplate the reasons for human existence or the meaning of life.                                        10000 non-null  int64 \n",
      " 47  Your emotions control you more than you control them.                                                                 10000 non-null  int64 \n",
      " 48  You take great care not to make people look bad, even when it is completely their fault.                              10000 non-null  int64 \n",
      " 49  Your personal work style is closer to spontaneous bursts of energy than organized and consistent efforts.             10000 non-null  int64 \n",
      " 50  When someone thinks highly of you, you wonder how long it will take them to feel disappointed in you.                 10000 non-null  int64 \n",
      " 51  You would love a job that requires you to work alone most of the time.                                                10000 non-null  int64 \n",
      " 52  You believe that pondering abstract philosophical questions is a waste of time.                                       10000 non-null  int64 \n",
      " 53  You feel more drawn to places with busy, bustling atmospheres than quiet, intimate places.                            10000 non-null  int64 \n",
      " 54  You know at first glance how someone is feeling.                                                                      10000 non-null  int64 \n",
      " 55  You often feel overwhelmed.                                                                                           10000 non-null  int64 \n",
      " 56  You complete things methodically without skipping over any steps.                                                     10000 non-null  int64 \n",
      " 57  You are very intrigued by things labeled as controversial.                                                            10000 non-null  int64 \n",
      " 58  You would pass along a good opportunity if you thought someone else needed it more.                                   10000 non-null  int64 \n",
      " 59  You struggle with deadlines.                                                                                          10000 non-null  int64 \n",
      " 60  You feel confident that things will work out for you.                                                                 10000 non-null  int64 \n",
      " 61  Personality                                                                                                           10000 non-null  object\n",
      "dtypes: int64(61), object(1)\n",
      "memory usage: 4.7+ MB\n"
     ]
    }
   ],
   "source": [
    "## Explore the dataset (e.g., size, features, target variables, summary statistics).\n",
    "## Check for any missing values and handle them if necessary.\n",
    "\n",
    "shape = df_personality.shape\n",
    "print(df_personality.columns)\n",
    "print(df_personality.describe())\n",
    "df_personality.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqA4aCvXs_xx"
   },
   "source": [
    "### 2.3. Data Preprocessing (5 points)\n",
    "* Explain the preprocessing steps taken and their rationale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In preprocessing first I drop the ID columns because it has no effect on the result and then I label the personality types with sequential numbers to work on data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EqPp7woss_xx"
   },
   "outputs": [],
   "source": [
    "## Perform any necessary data transformations or feature engineering.\n",
    "\n",
    "# normalize  into (-1,1) range\n",
    "def min_max_normalization(arr):\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    normalized_arr = 2 * ((arr - min_val) / (max_val - min_val)) - 1\n",
    "    return normalized_arr\n",
    "\n",
    "unique_values = df_personality.iloc[:, -1].unique()\n",
    "df_personality['Personality'] = df_personality['Personality'].astype('category').cat.codes\n",
    "\n",
    "df_personality = df_personality.drop(['Response Id'] , axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cheking number of different class\n",
    "\n",
    "pers_matrix = df_personality.values\n",
    "\n",
    "row_number, column_number = pers_matrix.shape[0] , pers_matrix.shape[1]\n",
    "class_number = np.unique(pers_matrix[:,-1]).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data is splitted and column with all ones will be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NSGP9Tjps_xx"
   },
   "outputs": [],
   "source": [
    "## Handle missing values (if any).\n",
    "## Split the dataset into training and testing sets. (80% train, 20% test)\n",
    "\n",
    "\n",
    "# spliting data to train and test data \n",
    "trainsize = int(4*(row_number/5))\n",
    "testsize = row_number-trainsize\n",
    "\n",
    "x_train = pers_matrix[:trainsize, : -1]\n",
    "y_train = pers_matrix[:trainsize, -1 ]\n",
    "\n",
    "x_test = pers_matrix[trainsize:, :-1 ]\n",
    "y_test = pers_matrix[trainsize: , -1 ]\n",
    "\n",
    "# making first column all ones in features matrix to multiply with bias\n",
    "bias_column = np.ones((x_train.shape[0],1))\n",
    "x_train = np.hstack((bias_column,x_train))\n",
    "\n",
    "bias_column = np.ones((x_test.shape[0],1))\n",
    "x_test = np.hstack((bias_column,x_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AUPSGkss_xx"
   },
   "source": [
    "### 2.4. Implementing and Evaluating Perceptron for Linear Classification (10 points)\n",
    "* Explain the reason behind the application of perceptron learning algorithm on this type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perceptron is faster compared to other more complex algorithms. It can be trained quickly, especially on large datasets like this one, which consists of 10 thousand rows. Therefore, using such an algorithm makes perfect sense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ujz4g2mJs_xx"
   },
   "outputs": [],
   "source": [
    "## Implement the Perceptron learning algorithm from scratch.\n",
    "\n",
    "def PerceptronLearning(matrix, labels , L , iteration):\n",
    "    \n",
    "    ## creating a weight matrix to make dot product and find highest score with all columns\n",
    "    weight_matrix = np.zeros((class_number,matrix.shape[1]))\n",
    "    \n",
    "\n",
    "    for iterate in range(iteration): \n",
    "        for row in range(matrix.shape[0]):\n",
    "            # trying to find the highest score\n",
    "\n",
    "            score = 0\n",
    "            predicted_class = 0\n",
    "            true_class = labels[row]\n",
    "            for w_row in range(weight_matrix.shape[0]):\n",
    "                row_score = np.sum(np.multiply(matrix[row],weight_matrix[w_row]))\n",
    "                if(row_score>score):\n",
    "                    score = row_score\n",
    "                    predicted_class = w_row\n",
    "            \n",
    "            # update rule for perceptron algorithm\n",
    "            if predicted_class != true_class:\n",
    "                weight_matrix[true_class] += L * matrix[row]\n",
    "                weight_matrix[predicted_class] -= L * matrix[row]\n",
    "    \n",
    "    return weight_matrix\n",
    "            \n",
    "\n",
    "def predict(matrix , weights , tried_row):\n",
    "    # making predictions with using weight matrix already created\n",
    "    \n",
    "    max_score = 0\n",
    "    predicted_class = 0\n",
    "    for w_row in range(weights.shape[0]):\n",
    "        row_score = np.sum(np.multiply(matrix[tried_row],weights[w_row]))\n",
    "        if(row_score > max_score):\n",
    "            max_score=row_score\n",
    "            predicted_class = w_row\n",
    "\n",
    "    return predicted_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "cD_8QUZhs_xx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy :  0.886125\n"
     ]
    }
   ],
   "source": [
    "## Train the Perceptron model on the training set.\n",
    "\n",
    "trained_weights = PerceptronLearning(x_train,y_train,0.0001,50)\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "# calculating the correct and incorrect answers of the model to find accuracy\n",
    "for tried_value in range(x_train.shape[0]):\n",
    "    if(predict(x_train,trained_weights,tried_value) == y_train[tried_value]):\n",
    "        correct += 1\n",
    "    else :\n",
    "        incorrect += 1\n",
    "training_accuracy = correct / (correct+incorrect)\n",
    "print(\"Training accuracy : \" , training_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uMoNPAfes_xx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy :  0.8565\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model's performance on the test set by calculating accuracy. Comment on the score.\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "# calculating the correct and incorrect answers of the model to find accuracy\n",
    "\n",
    "for tried_value in range(x_test.shape[0]):\n",
    "    if(predict(x_test,trained_weights,tried_value) == y_test[tried_value]):\n",
    "        correct += 1\n",
    "    else :\n",
    "        incorrect += 1\n",
    "test_accuracy = correct / (correct+incorrect)\n",
    "print(\"Test accuracy : \" , test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training accuray is 0.88 , test accuracy is 0.85. There is no overfitting or underfitting in data the decreasing value in accuracy is acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6P7NdYbs_xx"
   },
   "source": [
    "### 2.5 Evaluating SVM for Linear Classification (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "s46C2XOls_xx"
   },
   "outputs": [],
   "source": [
    "## Import SVM model from scikit-learn.\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IKvrvorGs_xy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainin accuracy :  0.991125\n"
     ]
    }
   ],
   "source": [
    "## Train the SVM model on the training set.\n",
    "\n",
    "svm_model = svm.SVC(gamma = 'auto')\n",
    "svm_model.fit(x_train,y_train)\n",
    "\n",
    "train_accuracy_svm = svm_model.score(x_train,y_train)\n",
    "print(\"Trainin accuracy : \" , train_accuracy_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "seZgrBZzwotd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.984\n"
     ]
    }
   ],
   "source": [
    "## Evaluate the model's performance on the test set by calculating accuracy. Comment on the score.\n",
    "\n",
    "predictions = svm_model.predict(x_test)\n",
    "test_accuracy_svm = np.mean(predictions == y_test)\n",
    "print(\"Test accuracy:\", test_accuracy_svm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can take really fast results and really good accuracy in built-in SVM. Built-in SVM method is too much optimized compared to my Perceptron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biAtQZdWs_xy"
   },
   "source": [
    "### 2.6 Results Analysis and Conclusion (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4DYImwx7s_xy"
   },
   "outputs": [],
   "source": [
    "## Compare the performance of Perceptron and SVM models in terms of accuracy.\n",
    "## Discuss the strengths and limitations of each model.\n",
    "## Summarize key findings from the classification analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  In accuracy scores Perceptron gives 0.85 accuracy SVM gives 0.98 in test set so for this dataset choosing SVM can be more logical if accuracy is the first priority.\n",
    "\n",
    "* Generaly Perceptron Model is faster than SVM Model but because of this is my implementation from scratch and it is not optimized too much Perceptron is running slower. Also in terms of advantages of SVM , SVM can work on non-linear classification problems which Perceptron can't. \n",
    "\n",
    "* For this dataset our key finding must be SVM is working with more accuracy than Perceptron model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Thank you for reading my code and report. I hope you like my Assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Qb4QIkS68RB4",
    "9-LA2-Ww8RB6",
    "ge1YGgKT8RB7",
    "O6yQkfFD8RB8",
    "23njcboArttc",
    "3aW87Td98RB9",
    "Jfa8FPbS8RCC",
    "8Y4hOS8Js_xw",
    "cqA4aCvXs_xx",
    "7AUPSGkss_xx",
    "H6P7NdYbs_xx",
    "biAtQZdWs_xy"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
